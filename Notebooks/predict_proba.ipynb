{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, plot_roc_curve, accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import XyScaler\n",
    "from roc_curve2 import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event1 = pd.read_csv('/home/allen/Galva/capstones/capstone2/data/Train/First_Health_Camp_Attended.csv')\n",
    "event2 = pd.read_csv('/home/allen/Galva/capstones/capstone2/data/Train/Second_Health_Camp_Attended.csv')\n",
    "event3 = pd.read_csv('/home/allen/Galva/capstones/capstone2/data/Train/Third_Health_Camp_Attended.csv')\n",
    "patient_df = pd.read_csv('/home/allen/Galva/capstones/capstone2/data/patient_attendance.csv')\n",
    "patient_df1 = patient_df.copy()\n",
    "print(patient_df1['Patient_ID'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/allen/Galva/capstones/capstone2/data/Train/Train.csv')\n",
    "train1 = train.copy()\n",
    "test_df = pd.read_csv('/home/allen/Galva/capstones/capstone2/data/test.csv') \n",
    "test_df1 = test_df.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df1['Education_Score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# counts = [848,1019,886,813,37633]\n",
    "# count_labels = ['Online_Follower','Facebook_Shared','LinkedIn_Shared','Twitter_Shared', 'Total' ]\n",
    "\n",
    "# ax.bar(range(len(counts)), counts)\n",
    "# ax.set_xticks(range(len(counts)))\n",
    "# ax.set_xticklabels(count_labels, rotation=45);\n",
    "# plt.figsize(8,8)\n",
    "# plt.savefig('Online_Sharing_Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patient_df1[['Online_Follower','Facebook_Shared','LinkedIn_Shared','Twitter_Shared']].hist(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train.columns)\n",
    "print(test_df.columns)\n",
    "print(patient_df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_id = patient_df1.Patient_ID.values\n",
    "pp_id = test_df.Patient_ID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_discolusure(pp_id,p_id):\n",
    "    print(f'length of train is {len(p_id)} and length of patient is {len(pp_id)} ')\n",
    "    p1,p2 = set(p_id) , set(pp_id)\n",
    "    print(f' unique for train is {len(p1)} and unique for patient is {len(p2)} ')\n",
    "    d = p1.difference(p2)\n",
    "    print(f'The difference from p1 to p2 is {len(d)}')\n",
    "    dd = p2.difference(p1)\n",
    "    print(f'The difference from p2 to p1 is {len(dd)}')\n",
    "    u, uu = p1.intersection(p2), p2.intersection(p1)\n",
    "    return len(u),len(uu)\n",
    "print(full_discolusure(pp_id,p_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train1.merge(patient_df1, on=['Patient_ID'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1 = test_df1.merge(patient_df1, on = ['Patient_ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1[train1['Patient_ID'] == 489652 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train1 = train1.drop(['Unnamed: 0', 'Unnamed: 0.1','Registration_Date', 'Employer_Category','Patient_ID','First_Interaction', 'Health_Camp_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1a = train1.drop(['Health_Camp_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1 = test_df1.drop(['Unnamed: 0', 'Unnamed: 0.1','Registration_Date', 'Employer_Category','Patient_ID','First_Interaction','Health_Camp_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1a = test_df1.drop(['Health_Camp_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, y2 = train1.pop('Event1_or_2') , test_df1.pop('Event1_or_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test = train1 , test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 , X_test2 = train1a , test_df1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression(solver=\"lbfgs\")\n",
    "logmodel.fit(X_train, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel2 = LogisticRegression(solver=\"lbfgs\")\n",
    "logmodel2.fit(X_train2, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logmodel.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = logmodel2.predict_proba(X_test2)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, preds2 = predictions>.55 ,predictions>.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsa, preds2a = predictions_2>.55 ,predictions_2>.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, coef in zip(preds2[1:], logmodel.coef_[0]):\n",
    "    print(\"{0}: {1:0.4f}\".format(name, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y2,preds2) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y2,predictions_2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y2,preds2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y2,preds) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools import add_constant\n",
    "from statsmodels.discrete.discrete_model import Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df1\n",
    "X_const = add_constant(X, prepend=True)\n",
    "y3 = y2.copy()\n",
    "logit_model = Logit(y3, X_const).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train):\n",
    "    model = LogisticRegression(solver=\"lbfgs\")\n",
    "    model.fit(X_train.iloc[train_index], y1.iloc[train_index])\n",
    "    y_predict = model.predict(X_train.iloc[test_index])\n",
    "    y_true = y1.iloc[test_index]\n",
    "    accuracies.append(accuracy_score(y_true, y_predict))\n",
    "    precisions.append(precision_score(y_true, y_predict))\n",
    "    recalls.append(recall_score(y_true, y_predict))\n",
    "\n",
    "print(\"Accuracy:\", np.average(accuracies))\n",
    "print(\"Precision:\", np.average(precisions))\n",
    "print(\"Recall:\", np.average(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "n=20 \n",
    "Accuracy: 0.6178722087906098\n",
    "Precision: 0.6242284105951045\n",
    "Recall: 0.9587480176361393\n",
    "'''\n",
    "#[[16088  2418]\n",
    " #[ 9349  7394]]\n",
    "'''\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.63      0.87      0.73     18506\n",
    "           1       0.75      0.44      0.56     16743\n",
    "\n",
    "    accuracy                           0.67     35249\n",
    "   macro avg       0.69      0.66      0.64     35249\n",
    "weighted avg       0.69      0.67      0.65     35249\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_plot(ax, x, y, x_label, y_label, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model3 = LogisticRegression(solver='lbfgs')\n",
    "model3.fit(X_train, y1)\n",
    "probabilities2 = model3.predict_proba(X_test)[:, 1]\n",
    "tpr, fpr, thresholds = roc_curve(probabilities2, y2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "simple_plot(ax, fpr, tpr, \n",
    "            \"False Positive Rate (1 - Specificity)\", \n",
    "            \"True Positive Rate (Sensitivity, Recall)\", \n",
    "            \"ROC Plot of Patient Predictions OUT of BOX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.63      0.87      0.73     18506\n",
    "           1       0.75      0.44      0.56     16743\n",
    "\n",
    "    accuracy                           0.67     35249\n",
    "   macro avg       0.69      0.66      0.64     35249\n",
    "weighted avg       0.69      0.67      0.65     35249\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_score = (2* .63 * .87)/(.63+.87)\n",
    "print(f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.73      0.35      0.48     18506\n",
    "           1       0.54      0.85      0.67     16743\n",
    "\n",
    "    accuracy                           0.59     35249\n",
    "   macro avg       0.64      0.60      0.57     35249\n",
    "weighted avg       0.64      0.59      0.57     35249\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 200)\n",
    "rfc.fit(X_train,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_preds = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y2,rfc_preds) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = RandomForestClassifier(n_estimators = 200)\n",
    "model4.fit(X_train, y1)\n",
    "proba_Forest = model4.predict_proba(X_test)[:, 1]\n",
    "tpr, fpr, thresholds = roc_curve(proba_Forest, y2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "simple_plot(ax, fpr, tpr, \n",
    "            \"False Positive Rate (1 - Specificity)\", \n",
    "            \"True Positive Rate (Sensitivity, Recall)\", \n",
    "            \"ROC Plot Patient Predictions Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
